# https://cobalt-clock-cf4.notion.site/2-d7701792858d4f029a257d80fbadc588
import streamlit as st
import os
from typing import Optional
from langchain_core.documents.base import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.vectorstores.base import VectorStoreRetriever
from langchain_community.vectorstores import FAISS
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from dotenv import load_dotenv
load_dotenv() 
openai_api_key = os.getenv("OPENAI_API_KEY")

st.set_page_config(page_title="ëª¨ë‘í•´ìœ ",
                   page_icon="ðŸ¤–")

st.title("ëª¨ë‘í•´ìœ  ê³ ê° ì‘ëŒ€ ì±—ë´‡ ë§Œë“¤ê¸°!") #h1

# st.markdown("""\n
# ëª¨ë‘í•´ìœ  ì„¸ë¯¸ë‚˜ì— ì˜¤ì‹ ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!\n
# ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.
# """) #markdown

# st.write("ëª¨ë‘í•´ìœ  ê³ ê° ì‘ëŒ€ ì±—ë´‡ ë§Œë“¤ê¸°!") #h2

# st.text("ëª¨ë‘í•´ìœ  ê³ ê° ì‘ëŒ€ ì±—ë´‡ ë§Œë“¤ê¸°!") #text


# íŒŒì¼ ìž„ë² ë”© ìˆ˜í–‰
@st.cache_resource(show_spinner="Loading...") # ì–˜ëŠ” ëº´ê³  í•´
def embedding_file(file: str) -> VectorStoreRetriever:
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=300,
        chunk_overlap=100,    # ë¬¸ì„œ ì‚¬ì´ì˜ ê³µë°±
        separators=["\n\n"],  # ë¬¸ë‹¨ ë‚˜ëˆ”
    )

    loader = PyPDFLoader(f"./files/{file}") # str => íŒŒì¼ ê²½ë¡œ
    docs = loader.load_and_split(text_splitter=splitter) # íŒŒì¼ ë¡œë“œ ë° ë¶„í• 

    embeddings = OpenAIEmbeddings() # ìž„ë² ë”© ìƒì„± 
    vector_store = FAISS.from_documents(docs, embeddings) # ë²¡í„°ìŠ¤í† ì–´
    retriever = vector_store.as_retriever() # ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ê°€ì§„ ë¦¬íŠ¸ë¦¬ë²„

    return retriever


with st.sidebar:
    chat_clear = None
    model_name = st.selectbox(label="ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", placeholder= "Select Your Model", options=["gpt-3.5-turbo","gpt4o"], index=None)
    file = st.selectbox(label="íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.", placeholder= "Select Your File", options=(os.listdir("./files")), index=None)
    if file:
        retriever = embedding_file(file)
        st.success(f"ìž„ë² ë”©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.info(f"í˜„ìž¬ ìž„ë² ë”© ëœ íŒŒì¼ëª… : \n\n **{file}**")
        col1, col2 = st.columns(2)
        chat_clear = col1.button("ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”")
        embed_clear = col2.button("ìž„ë² ë”© ì´ˆê¸°í™”")

        llm = ChatOpenAI(temperature=0.1,
                         model_name=model_name)          

        if embed_clear:
            st.cache_resource.clear()
        
if not file:
    st.markdown("""\n
    ëª¨ë‘í•´ìœ  ì„¸ë¯¸ë‚˜ì— ì˜¤ì‹ ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!\n
    ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.
    """)

if "history" not in st.session_state:
    st.session_state["history"] = []
    
def chat_history(message=None, role=None, show=True):
    if message and role:
        st.session_state["history"].append({"message":message, "role":role})   
    if show:
        for m in st.session_state["history"]:
            with st.chat_message(m["role"]):
                st.write(m["message"])


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", 
     """
    context : {context}

    ë‹¹ì‹ ì€ ì–¸ì œë‚˜ ê³ ê°ì—ê²Œ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€ì„ í•˜ë©° ë§íˆ¬ëŠ” êµ‰ìž¥ížˆ ì¹œê·¼í•©ë‹ˆë‹¤. ì§ì—…ì€ ì „ë¬¸ ìƒë‹´ì›ìž…ë‹ˆë‹¤. ë‹µë³€ ì‹œ, ì•„ëž˜ì˜ ê·œì¹™ì„ ì§€ì¼œì•¼ë§Œ í•©ë‹ˆë‹¤.
    ê·œì¹™ 1. ì£¼ì–´ì§„ contextë§Œì„ ì´ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼í•©ë‹ˆë‹¤. 
    ê·œì¹™ 2. ì£¼ì–´ì§„ contextì—ì„œ ë‹µë³€ì„ í•  ìˆ˜ ì—†ë‹¤ë©´ "í•´ë‹¹ ë¬¸ì˜ëŠ” 010-8746-5839ìœ¼ë¡œ ì—°ë½ì£¼ì‹œë©´ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì˜ì—… ì‹œê°„ì€ ì˜¤ì „ 10ì‹œ-ì˜¤í›„ 6ì‹œìž…ë‹ˆë‹¤." ë¼ê³  ëŒ€ë‹µí•˜ì„¸ìš”.
    ê·œì¹™ 3. ë¬¸ìžì—´ì— A1, A2, A11, A22 ë“± í•„ìš” ì—†ëŠ” ë¬¸ìžëŠ” ì œê±°í•œ ë’¤ ì¶œë ¥í•©ë‹ˆë‹¤.
    ê·œì¹™ 4. í•­ìƒ ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì‘ëŒ€í•©ë‹ˆë‹¤.
    ê·œì¹™ 5. ì›¹ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. ëŒ€ì†Œë¬¸ìžë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì„¸ìš”.
    """),
    ("human", "{query}")
])


query = st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”.")

def format_docs(docs: list[Document]) -> str:
    return "\n\n".join(doc.page_content for doc in docs)


def chat_llm(query: str) -> None:
    chat_history(message=query, role="user", show=False)    
    chain = {"context":retriever | RunnableLambda(format_docs),
             "query":RunnablePassthrough()} | prompt | llm
    result = chain.invoke(query)
    chat_history(message=result.content, role = "ai")    

if query:
    chat_llm(query)

